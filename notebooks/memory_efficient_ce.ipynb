{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MemoryEfficientCrossEntropy(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, W, targets):\n",
    "        \"\"\"\n",
    "        Custom forward pass without storing logits (z = x @ W)\n",
    "        \"\"\"\n",
    "        logits = x @ W  # Compute logits (not stored)\n",
    "        loss = F.cross_entropy(logits, targets)  # Compute loss\n",
    "\n",
    "        # Store only necessary values for backprop\n",
    "        ctx.save_for_backward(x, W, targets)\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        Custom backward pass to compute gradients without storing logits.\n",
    "        \"\"\"\n",
    "        x, W, targets = ctx.saved_tensors  # Retrieve saved tensors\n",
    "\n",
    "        # Compute logits again (since we didn't store them)\n",
    "        logits = x @ W\n",
    "        probs = F.softmax(logits, dim=-1)  # Softmax probabilities\n",
    "\n",
    "        # Compute gradient of cross-entropy loss\n",
    "        probs[range(len(targets)), targets] -= 1  # One-hot adjustment\n",
    "        probs /= len(targets)  # Normalize gradient\n",
    "        grad_x = probs @ W.T  # Gradient w.r.t x\n",
    "        grad_W = x.T @ probs  # Gradient w.r.t W\n",
    "\n",
    "        return grad_x * grad_output, grad_W * grad_output, None\n",
    "\n",
    "# Example test case\n",
    "batch_size, input_dim, num_classes = 32, 512, 1000\n",
    "x = torch.randn(batch_size, input_dim, requires_grad=True)\n",
    "W = torch.randn(input_dim, num_classes, requires_grad=True)\n",
    "targets = torch.randint(0, num_classes, (batch_size,))\n",
    "\n",
    "# Use memory-efficient loss function\n",
    "loss = MemoryEfficientCrossEntropy.apply(x, W, targets)\n",
    "\n",
    "# Backpropagation\n",
    "loss.backward()\n",
    "\n",
    "print(\"Loss computed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
